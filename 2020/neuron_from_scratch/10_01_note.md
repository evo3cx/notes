# Optimizer (Chapter 10)

Table of contents:

- [Optimizer (Chapter 10)](#optimizer-chapter-10)
  - [Learning Rate Decay](#learning-rate-decay)
    - [How Learning Rate Decay works](#how-learning-rate-decay-works)
  - [Stochastic Gradient Descent with Momentum](#stochastic-gradient-descent-with-momentum)
  - [AdaGrad](#adagrad)
  - [RMSProp](#rmsprop)


-------------------------------

## Learning Rate Decay

Page 31
The idea of a learning rate decay is to start with a large learning rate, say 1.0 in our case, and then decrease it during training.

### How Learning Rate Decay works

We're hoing to udpate the learning rate each step by the reciprocal of step count fraction.
This fractions is a new hyper-parameter that we'll add to the optimizer, and this new hyper parameter called "learning rate decay"


implementation it takes the step and the decaying ration and multiplication them. The further in training, the bigger the step is and the bigger result of this multiplication

We then take its reciprocal (the further in training, the lower the value) and multiply the initial learning rate by it.

Then added 1 make sure that the resulting alhorithm never raises the learning rate. For example, for the first step, we might devide 1 by the learning rate, 0.001 for example, which will result in current learning rate of 1000. That's definitely not what we wanted.

1 diveded by the 1+fraction ensure that the result, a fraction of the starting learning rate, will always be less than or equal to 1, decreasing over time. that's the desired result

This learning rate decay scheme lower the learning rate each step using the mentioned formula. 
Initially, the learning rate drops fast, but the change in the learning rate lowers each step, letting the model sit as close as possible to the minimum. The model needs small updates near the end of training to be able to get as close to this point as posible. 

------------------------

## Stochastic Gradient Descent with Momentum

Momentum creates a rolling average of gradients over some number of updates and uses this average with the unique gradient at each step.

Another way of understanding this is to imagine a ball going down a hill — even if it finds a small hole or hill, momentum will let it go straight through it towards a lower minimum — the bottom of this hill. 

This can help in cases where you’re stuck in some local minimum (a hole), bouncing back and forth.


-------------------

## AdaGrad

AdaGrad, short for **Adaptive Gradient**, institues a pre-parameter learning rate rater than a globallly-sared rate. 

The idea here is to normalize updates made to the features. During the training process, __some weights can rise significantly, while others tend to not change by much__. It is usually better for weights to not rise too high compared to the other weights, and we'll talk about this with regularization techniques.

AdaGrad provides a way to normalize parameter updtes by keeping a history of previous updates -- the bigger the sum of the updates is, in either direction (positive or negative), the smaller updates are made further in training.

The concept of AdaGrad can be contained in the following two lines of code:

```
cache += param_gradient ** 2
param_updates = learning_rate * parm_gradient / (sqrt(cache) + eps )

```

- *cache* holds a history of squared gradients
- *parm_updates* is a function of learning rate multipled by gradient (basic SGD so far) 
- *epsilon* its a *hyperparamter* (pre-training control knob setting) preventing division by 0.


-------------------


## RMSProp

RMSProps short for **Root Mean Square Propagation**. similiar to **AdaGrad**, RMSProp calculates an adaptive learning rate per parameter; it's just calculated in a different way than **AdaGrad**.

When AdaGrad calculate the cache as:

```
cache += gradient ** 2
```

RMSprops calcualte the cache as:

```
cache = rho * cache + (1 - rho) * gradient ** 2
```

- *rho* its hyeperparameter the cache memory decay rate
- *cache* holds a history of squared gradients

Note that this is similar to both momentum with the SGD optimizer and cache with the AdaGrad. RMSProp adds a mechanism similar to momentum but also adds a per-parameter adaptive learning rate, so the learning rate changes are smoother. 

This help to retain the global direction fo changes and slows chaanges in direction. Instead of continually adding squared gradeitn to a cache (like in Adagrad), it uses a moving average of the cache. Each update to the cache retains a part of the cache and udpate it with a fraction of the new, squred, gradients. In this way, cache contents "move" with data in time, and learning does not stall. In the case of this optimizer, the per-parameter learning rate can either fall or rise, depnding on the last updates and current gradient. RMSProps applies the cache in the same way as **AdaGrad** does.


