# Optimizer (Chapter 10)

# Learning Rate Decay

Page 31
The idea of a learning rate decay is to start with a large learning rate, say 1.0 in our case, and then decrease it during training.

### How Learning Rate Decay works

We're hoing to udpate the learning rate each step by the reciprocal of step count fraction.
This fractions is a new hyper-parameter that we'll add to the optimizer, and this new hyper parameter called "learning rate decay"


implementation it takes the step and the decaying ration and multiplication them. The further in training, the bigger the step is and the bigger result of this multiplication

We then take its reciprocal (the further in training, the lower the value) and multiply the initial learning rate by it.

Then added 1 make sure that the resulting alhorithm never raises the learning rate. For example, for the first step, we might devide 1 by the learning rate, 0.001 for example, which will result in current learning rate of 1000. That's definitely not what we wanted.

1 diveded by the 1+fraction ensure that the result, a fraction of the starting learning rate, will always be less than or equal to 1, decreasing over time. that's the desired result

------------------------

This learning rate decay scheme lower the learning rate each step using the mentioned formula. 
Initially, the learning rate drops fast, but the change in the learning rate lowers each step, letting the model sit as close as possible to the minimum. The model needs small updates near the end of training to be able to get as close to this point as posible. 


# Stochastic Gradient Descent with Momentum

Momentum creates a rolling average of gradients over some number of updates and uses this average with the unique gradient at each step.
