## Activation Function

Only use linear activation function in y hat or output Layer, But if y takes on these real values, then it might be okay to have a linear activation function here so that your output y hat is also a real number going anywhere from minus infinity to plus infinity.

**But then the hidden units should not use the activation functions**. They could use ReLU or tanh or Leaky ReLU or maybe something else.

- [Exercise](exercise/2.activation_relu.py)
